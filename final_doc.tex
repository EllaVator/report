\documentclass[a4paper, 12pt]{article}
\usepackage[english]{babel}
\usepackage[a4paper,left=2.5cm, right=2.0cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{url}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{lscape}
\usepackage[final]{pdfpages}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage[nottoc,notlot,notlof]{tocbibind}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
%This file will subsume all of the individual chapters of the project report


%\setstretch{1.50}
\pagestyle{empty}
\include{title_page}
\pagestyle{plain}
\setcounter{page}{1}
\tableofcontents
\cleardoublepage
%please put in the files you wrote in the corresponding order and space by
%using \input{filename} so that we can compile the entire project report as one

\section{Components of elevator control}

\subsection{Automatic speech recognition}
Speech recognition is a complex technology which involves many fields, such as probability theory, machine learning, statistical natural language processing and speech processing. However, the architecture of a modern speech recognizer can be broadly divided into two main components namely the Acoustic Model (AM) and the Language Model (LM). The decoder uses these two components and produces the text output of the extracted input features as shown in Figure~\ref{fig:asr_component1}. 
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{ASR_Component1.pdf}
\vspace{-1.2cm}
\caption{\textit{Basic components of speech recognition system.}}
\label{fig:asr_component1}
\end{figure}


If an utterance is represented as a sequence of words, $W$ = $w_1$, $w_2$, $w_3$, \dots \dots, $w_N$, then the decoder will try to find the most probable word sequence $\hat{W}$, given the sequence of acoustic vectors, $Y$ = $y_1$, $y_2$, \dots \dots, $y_T$. Using Byes rule, the probability of a word given an observation $P(W/Y)$ can be found as: 
\begin{equation}
\label{equ:bayes_rule}
\hat{W} = \argmax P(W/Y) = \argmax\frac{P(W)P(Y/W)}{P(Y)}.
\end{equation}
In this equation, the word sequence $\hat{W}$ must be found, as the one that maximizes the product of $P(W)$ and $P(Y/W)$, where $P(W)$ is the prior probability which comes from the language model and $P(Y/W)$ is the probability of vector sequence $Y$ given some word sequence $W$, which is known from the acoustic model. $P(Y)$ is ignored, as it is a constant for all possible word sequences. Below each of the component is explained in more detail.
\subsubsection{Acoustic modeling}
The performance of the speech recognition systems mostly depends on a good acoustic model. Acoustic modeling for speech recognition refers to the process of how one can statistically represent the features vectors sequences, which are computed from the continuous speech. Since, it is difficult to have frequent number of each word in the training data to accumulate acoustic statistics for that word, therefore, each word is represented into smaller units of speech called the phonemes. This is usually called the  "pronunciation modeling". In modern speech recognition systems, approximately 39-45 phonemes are used, which are combined to form all possible words in English. To model the acoustic features, Hidden Markov Model (HMM)~\cite{hmm_for_SR} are used.

\subsubsection{Language modeling}
The language modeling (LM) specifies what sequence of words to expect in the speech given the previous word or sequence of $N$ words. In equation~\ref{equ:bayes_rule}, the term $P(W)$, represents the language model. Many techniques have been used to estimate the probability, $P(W)$. Two of the most important techniques are mentioned below.
\subsubsection*{Formal language models} 
\label{sec:gramar_based_lm}
The formal or grammar based language model is composed of a grammar and a parsing algorithm. The grammar specifies allowable structure of a language and the parsing specifies the analysis method to check if rules of the grammar are respected. For small size recognition task, often the grammar based language model is a good solution. 
\subsubsection*{Probabilistic language models}
\label{sec:prob_lang_model}
The probabilistic language models assigns probabilities to words $\textit{W}_k$ based on preceding words $\textit{W}_k^{k-1} = \textit{w}_1 , \cdots \cdot,\textit{w}_{k-1}$  from a training corpus. The most commonly used language model from this category are the $N$-gram language models, where $N$ means number of preceding words to be considered. Some variations are \textit{unigrams}, \textit{bigrams} and \textit{trigrams}. 

For \textit{trigrams}, where \textit{N=3} we can calculate the probability as:
\begin{equation}
  \hat{P}(w_k | w_{k-1},w_{k-2}) = \frac{c(w_{k-2},w_{k-1},w_k)}{n(w_{k-2},w_{k-1})} 
\end{equation}
where $c(w_{k-2},w_{k-1},w_k)$, specifies the number of times, the words $w_{k-2},w_{k-1}$ and $w_k$ comes together and $n(w_{k-2},w_{k-1})$, specifies the number of times the words $(w_{k-2},w_{k-1})$ occurs together. 

\subsection{Language model for elevator}
For the elevator, we have used grammar based language model, since the vocabulary size is small and the possible commands to the elevator can be modelled by the grammar. 
We wrote a simple grammar in the JSGF format\footnote{http://www.w3.org/TR/jsgf/}, that is usable with Sphinx.

\subsection{Reverberation recordings in the elevator}

\begin{figure}
\center{\includegraphics[scale=0.10, angle=-90]{setup_reverberation_rec.jpg}}
\caption{Recordings setup.}
\label{fig:recordingsetup}
\end{figure}


To train the acoustic model for the elevator, recordings of elevator-specific commands and phonetically balanced sentences in both English and German were made. 
Lists one and two  of the Harvard sentences\footnote{http://www.cs.columbia.edu/~hgs/audio/harvard.html} were used as phonetically balanced sentences in English and twenty sentences taken from BITS\footnote{http://www.bas.uni-muenchen.de/forschung/Bas/BasBITSUSTABLE} were used for German. 
The full list of sentences which have been used can be found in the appendix. \todo{TODO: attach appendix} %appendix Sätze
The sentences were read by all of the participants and recorded in a sound proof studio. 
This resulted in recordings of English sentences spoken by non-natives and German sentences spoken by natives and by non-natives.


The commands that are spoken inside the elevator, are taken via a cable to the system that are then processed. But during this process some distortion is produced. This distortion might be caused by cable or some other external factor. To account for this noise of the elevator and the sound of the opening/closing of doors and other noises from outside of the elevator, further recordings were made via the elevator's built-in microphone. The setup used for this can be seen in Figure \ref{fig:recordingsetup}.

It consisted of two loudspeakers positioned on a music stand at the height of approximately 150cm, at a distance of approximately 20cm from the elevator's microphone. 
The recordings made in the lab were played through the loudspeakers and recorded with Praat via the elevator's microphone. 
While the recordings were being played, the elevator was moved between the floors, its doors were opened and closed and kept stationary with its doors opened and closed on different floors.

\newpage
\section{General thoughts}
Before actually starting to work on the dialogue itself, we decided on how a dialogue with the EllaVator should look like. \\

We came up with the following model: \\


\begin{figure} [ht]
  \center{\includegraphics[scale=0.8, angle=0]{Dialogue_model.jpeg}}
  \caption{A model for the Dialogue}
  \label{fig: Dialogue flow}
\end{figure}

In the figure shown above commands from the user are marked in yellow, the answers from the EllaVator are marked in blue.
Here is a table giving example sentences for the states indicated by the figure above: \\


\begin{tabular}{|ll|}
  \hline
  Greeting & The user greets the Ellevator with a keyword \\
    & and thus activates the system.  \\
  Greet + Offer & Hello, how can I help you? \\
  Request & Take me to floor/room XY/ Name\_of\_a\_person \\
  Grounding/Clarify & You'd like to be taken to ...?\\ 
   & I didn't get it, could you repeat that? \\
  Confirm & Yes \\
  Reject & No \\
  Execute & Okay I shall now take you to ... \\ 
   & *here the elevator should also start the action of moving* \\
  \hline 
\end{tabular}
\newline
Of course the actual system should be able to understand a variety of sentences and maybe alternate the answers but the dialogue itself can be kept as simple as it is shown in the graph.  \\

We thought about including some chatting with the user or small Easter eggs. 
We have therefore made measurements on how much time the average user spends in the elevator to determine how much time there is for the dialogue.
On average the elevator needed 40 seconds to go from the ground floor to the top and 21 seconds for going down. Opening the Doors takes 4 seconds, closing them 5.5 seconds.
With this in mind we decided to keep the dialogue as simple as possible as one of the reasons for everyone using the elevator is to be faster compared to taking the stairs and that maybe the likelihood of people using the speech interface would be lower if too much time was spend on such interactions.\\

Instead, we discussed whether we should enable the user to merge the state "Greet" with the state "Request" which would allow for the user to both activate the elevator and also enter where they would like to be taken to.
This would skip one state from the user as well as one step from the elevator.
Depending on the confidence of the user input we also considered leaving out the "Grounding" state to have a shorter dialogue.\\

In the end we implemented the system as was described in the graph above and decided that we should first try out how the system works on the real elevator before making further changes.\\

Another thought we had but have put aside to work on after we have seen our project running on the actual hardware was that it might be the case that more than one person enters the elevator and that these people might have different destinations.
Of course it would be possible to just run the dialogue with one person, take them to their destination and then run the dialogue again with the next person.
This approach might however be inconvenient. 
It might be more natural to enable the system to take more then one request.\\

Another problem that arose when thinking about the scenario of more than one person entering the elevator, was that they might not share the same language.
We decided to provide an English as well as a German version of our system.
So the problem was not only to allow for multiple inputs but also for multiple inputs in different languages.\\
\newline

\textbf{
  \begin{center}Switching between languages... \\
    \underline{Our thoughts on this are collected in ticket 80}\\
    @Naska \\
    @Laura \\
    does one of you feel like writing a little about this issue?
\end{center}}

\newpage
\section{Dialog manager}
The dialog manager is responsible for keeping track of the conversation and deciding the next move of the system for each input. The domain of our project is fairly limited. Most conversational situations consist of the user naming a location and the machine taking the action of moving to that location and reassuring the user that it had understood the command. This can described by deterministic If...then... statements. A dialog manager that models the conversation flow as an deterministic FSA is well-suited for our needs.
When choosing the dialog manager for our project, we considered the following factors: It had to be open source, preferably Java and well documented.
We were also looking for code that was actively maintained.
We reviewed a few potential contenders:
\begin{itemize}
\item[IrisTK] \hfill \\
IrisTK is a sophisticated dialog manager. The main focus of this dialog manager is multimodality i.e it can integrate input from multiple sensory inputs, for example speech and vision. The multimodal nature of this manager adds a lot of functionality which we would not have used, so we decided not to use it.
\item[InproTK] \hfill \\
The most exciting asset of this dialog manager was incremental processing.
The code is well maintained and actively developed. 
However, we failed to build a demo that uses incremental features due to the lack of documentation so we dropped this option.
\item[OpenDial] \hfill \\
OpenDial is probably the most popular open source dialog manager.
After successfully implementing a short domain-relevant demo, we opted for this dialog manager.
For more detailed description, see section ~\ref{sec:opendial}.
\end{itemize}

\subsection{OpenDial}
\label{sec:opendial}
We used Opendial to implement not only the dialogue manager but our main class: the dialogue system. 
Opendial handles the whole dialogue system, which consists of the automatic speech recognition (ASR), the dialogue manager and the text-to-speech (TTS) system. 
We used a collection of plugins to connect external plugins to the dialogue system: Sphinx 4 ~\cite{Walker:2004:SFO:1698193} for ASR and MaryTTS ~\cite{marytts} for text to speeche 
This means that Opendial takes care of updating the dialogue manager's variables on speech input (coming from the ASR module), and forwarding the dialogue manager's output to the text to speech module. 
The main component that has to be implemented for the dialogue manager is the domain file. \\

Like most dialogue managers Opendial also consists of three different components which interact to create a dialogue. 

The first component is responsible for the recognition of language. 
In OpenDial this component is called ”Natural Language Understanding model (NLU)”. 
The output of the first process is passed on to the next component which checks if there was an instruction given which action should be be performed upon that input. 
Such actions might be showing something on a GUI, starting a task or simply selecting an answer to the user's input. 
In OpenDial this part is called ”Dialog Manager”.
The third and last component is called ”Natural Language Generation module (NLG)” and is basically the opposite of the NLU. 
The NLU generates language from the output of the ”Dialog Manger”. 
Though the component is named NLG, the output of a dialogue system is not necessarily speech, but the output is often designed so that it can be easily used to generate speech with a TTS component. 
This is also the case for OpenDial. 
OpenDial also offers a GUI where the dialogue (input and output) is displayed without having to use further plugins.

\subsubsection{Domain}

All of the components mentioned above are located in the so called Domain file.
A dialogue designer working with OpenDial will, in most cases just have to work on this one file. 
For easier readability, it can of course be split into smaller files, which will then have to be imported into one single file. 


In this file the user's input will be first transformed into an XML structure and then further processed.
Some systems fill this structure with semantic information. 
OpenDial however basically passes variables through the components of the system. 
The dialogue designer can then assigns values to those variables which will ultimately determine the flow of the dialogue. 
There is a convention for the naming of the variables, they can however be named to the dialogue designers liking. 
As all information is being passed through variables the correct naming is important though. 
All three components have a slot ”trigger” at the very beginning of their structure. 
The value given to this slot has to be the name of all variables which are to activate this component.
For example giving the component this trigger: 
\textless model trigger=”a\_u” \textgreater will make it respond to all variables with the name ”a\_u”.


So the variables in OpenDial do not only carry the value through the system but their names also determinate which module should continue to process that variable.
The most common way to link them was described above: 
Having the NLU module process the user's input, the dialogue manager process that input and then responding using the NLG module.
In case there is no further processing needed one might also decide to directly link NLU module to NLG module. 
If the system should for example ask a question after having said something, it might also be possible that the dialogue designer might want to trigger the NLG twice, either linking NLU to NLG module or NLU module to the dialogue manager which again links back to the NLG module.


The Domain-file is first subdivided into three parts as they each represent one components these parts are called modules. 
The modules themselves are again subdivided into rules. Every rule in this file represents one command. 

\subsubsection{Natural Language Understanding module}

The first model to receive input is the ”Natural Language Understanding module” (NLU). 
Depending whether further plug-ins are used, this module will get its input either directly from the user or from the plug-in.
OpenDial NLU component is able to work with plain text input from the user, but in the case of the Ellavator project the user shall also be able to use speech commands to operate the elevator, therefore the NLU in this project receives input from the grammar file of the Sphinx plug-in.
The grammar of the Sphinx plug-in already does part of the work, which would be usually done by the NLU alone.
There is only a very limited amount of commands which the elevator is able to execute.
Each of those can however be triggered by various expressions, as different people will use different words and structures to express themselves.
Therefore a domain-file that should be capable of doing something should have at least one rule which can be activated by at least one expression. 


In the example shown below there is a rule which models the command of changing a direction.
A rule consists of at least one case-expression.
A case-expression itself consists of a ”condition” and an ”effect”.
 In the ”condition”, as the name already states the dialogue designer can define under which circumstances a case-expression will match the users input.
 These conditions are connected using a logical operator.
 In most cases one would want to use the conditions ”or” as either one of those expressions should trigger the rule.
 The ”effect” will be the value passed on to the dialogue manager.  
 As stated before this is done by assigning a variable of the proper name a value.
 In the case of the example below the variable is assigned the value of the function of changing to floor to the level of the given parameter. 



  \textless rule\textgreater \newline
  \indent \indent \textless  case \textgreater \newline
  \indent \indent \indent \textless condition operator=”or”  \textgreater \newline\indent \indent \indent \indent \textless if var=”u\_u” value=”go to second floor” relation=”contains”/  \textgreater \newline
  \indent \indent \indent \indent \textless if var=”u\_u” value=”take me up to the second floor” \newline
  \indent \indent \indent \indent relation=”contains”/ \textgreater \newline
  \indent  \indent \indent \indent \textless if var=”u\_u” value=”second floor” relation=”contains”/ \textgreater \newline
  \indent \indent \indent \textless /condition \textgreater \newline
  \indent \indent \indent \textless effect prob=”1” \textgreater \textless set var=”a\_u” value=”Request(second)” / \textgreater \newline
  \indent \indent \indent \textless /effect \textgreater \newline
  \indent \indent \textless /case \textgreater \newline
  \indent \indent \textless case \textgreater \newline
  \indent \indent \indent \textless condition operator=”or” \textgreater \newline
  \indent \indent \indent \indent \textless if var=”u\_u” value=”go to thrid floor” relation=”contains”/ \textgreater \newline
  \indent \indent \indent \indent \textless if var=”u\_u” value=”take me up to the third floor” relation=”contains”/ \textgreater \newline
  \indent \indent \indent \indent \textless if var=”u\_u” value=”third floor” relation=”contains”/ \textgreater \newline
   \indent \indent \indent \textless /condition \textgreater \newline
   \indent \indent  \textless effect prob=”1” \textgreater \textless set var=”a\_u” value=”Request(third)” / \textgreater \newline
   \indent \indent  \textless /effect \textgreater \newline
   \indent \indent \textless /case \textgreater \newline
   \indent \textless /rule \textgreater \newline


   In the example above one rule is shown.
   This one rule includes two different commands.
   They are gathered in one rule as they both are orders to switch a floor.
   The only difference is the floor level they have.
   Resulting in different parameters which are given to the function of the variable ”a\_u” in the ”effect” slot. 



   As stated before, every user will use different words to express themselves.
   To cover all possible utterances for a command is impossible.
   But the dialogue designer should try to at least cover the most common ones,  to make the dialogue more natural for the user.
   This part can also be done in the grammar of Sphinx.
   OpenDial offers a few possibilities to do so.
   The first possibility would be to simply list all the expressions as it is shown in the figure above.
   But as can easily be seen in the example above, there is only a small difference between the sentences.
   If one would like to cover all possible sentences by merely enlisting them the Domain file will not only get extremely huge it will also be hard to read.


   \begin{tabular}{|ll|}
     \hline
      a? & The word ”a” may or may not occur in the expression .  \\
     \hline
      (a \textbar b \textbar... \textbar x) & One of the symbols written  in the brackets has to occur.\\
     \hline
      (a \textbar b \textbar... \textbar x))? & One of the symbols written in the brackets may or may not occur.  \\
     \hline
   \end{tabular}
   \newline

   This table shows the expressions which can be used to structure a the NLU. \newline \newline

   In the example used above all three user-utterances could be shortened down using the expression:
   \textless if var=”u\_u” value=”(go \textbar take me) (up to the)?  second floor” relation=”contains”/ \textgreater

   Using this expressions is therefore recommended.

   \subsubsection{Dialogue Manager}

   If the variable which was assigned in the NLU module was named correctly, it should, in most cases, first be passed through the dialogue manager (DM).
   In the DM, the effect of a rule is triggered. A mapping occurs which links the input provided by the NLU to another structure that will trigger a reaction of the system.
   As was said in the last subchapter, values in OpenDial are saved within and passed through variables, the input from the NLU will therefore be assigned to another variable. \newline
   Just like all other modules, each command in the DM is divided into rules, each of them having a condition, when they are to be activated. 
   Therefore, in this component the output of the NLU component will be matched against all conditions in the DM. 
   As soon as one matching rule is found, that rule is triggered which sends the ”effect” of that rule.
   The effect of a rule is saved into a variable.
   If no further work, like printing information on a GUI is done, one could argue, that all the DM does is basically linking two variables, the one given in the ”condition” to the variable written in the ”effect”.
   Of course not all parameters do have to be listed.
   It is sufficient to merely put a placeholder in curly brackets to indicate that there is a parameter to be passed on. \newline


   \textless rule id=”Movement” \textgreater \newline
    \indent \indent \textless case \textgreater \newline
    \indent \indent \indent \textless condition \textgreater \newline
    \indent \indent \indent \indent \textless if var=”a\_u” value=”Request(\{x\})” / \textgreater \newline
     \indent \indent \indent \textless /condition \textgreater \newline
      \indent \indent \indent \textless effect util=”1” \textgreater \newline 
       \indent \indent \indent \indent \textless set var=”a\_m” value=”floor(\{x\})” / \textgreater \newline
        \indent \indent \indent \textless /effect \textgreater \newline
        \indent \indent \textless /case \textgreater \newline
        \indent \textless /rule \textgreater \newline

The figure above shows  a rule called ”Movement” which links the users input ”Request” with any parameter to the effect of that input.
The variable with the name ”a\_m” and the value ”floor” with the input of the Request is send.
The parameter specified in the placeholder ”x” written in curly brackets.
\subsubsection{Natural Language Generation module}
The last part of the system generates the speech. 
Without any further plug-ins this will be done by printing the text to the GUI.
As stated before, this component basically does the opposite of the Natural Language Understanding module. 
Instead of transforming the user's input into an XML structure, the system takes the last components output, which is a variable embedded in an XML structure and transforms it back to plain text, either spoken, if plug-ins are used, or printed on the GUI, if no plug-ins are used. \newline

Each command is again given its own ”rule” and ”case” structure, in which condition and effect are included. 
The effect of this rule is saved in a variable but also used as output for speech. \newline

\textless rule \textgreater \newline
\indent \indent \textless case \textgreater \newline
\indent \indent \indent \textless condition \textgreater \newline 
\indent \indent \indent \indent \textless if var=”a\_m” value=”floor(\{x\})” / \textgreater \newline
\indent \indent \indent \textless /condition \textgreater \newline
\indent \indent \indent \textless effect util=”1” \textgreater \newline 
\indent \indent \indent \indent \textless set var=”u\_m” value=”Okay, I will now take you to the {x} floor.” / \textgreater \newline
\indent \indent \indent \textless /effect \textgreater \newline
\indent \indent\textless /case \textgreater \newline
\indent \textless /rule \textgreater \newline

The example above shows one rule from an NLG module. 
In this rule, a variable to which a function  is assigned, triggers the speech output of an acknowledgement from the system.
        
\newpage        
\section{Speech synthesis}
The final step in creating the speaking elevator "Ella" was of course to make it actually talk to the user.
For this a Text-to-Speech system (TTS) had to be used. A TTS system will read out information to the user, generating voice for the input text. TTS systems are widely used today. 
For exmaple in the train stations, there are announcements at the train station informing guests about delays or if there are changes causing a train to depart from a different platform as planned.
There are screen readers for the visual impaired, which read out the elements of a screen, as well as systems which are used by the speech impaired to communicate and speak in their stead.
One of the most famous applications is the dialogue system "Siri" (Siri inc) which has been used since 2011 on the iPhone. \newline \newline
For our talking elevator we wanted to use an open source TTS. 
We choose the MaryTTS (\textbf{M}odular \textbf{A}rchitecture for \textbf{R}easearch on speech s\textbf{Y}nthesis") system for our task.
Not only was it developed in cooperation with the University of Saarlandn, it also offers many advantages to other TTS and therefore seemed perfect for our task.\newline

Written in the programming language Java it was designed in a collaboration between the  "Language Technology Lab" of the DFKI \footnote{(\textbf{D}eutsches-\textbf{F}orschungszentrum für \textbf{K}ünstliche \textbf{I}ntelligenz) - The German Research Centre for Artificial Intelligence} and the faculty for phonetics of the university of Saarbrücken.
As MaryTTS is an open Source project, the code can be read by everyone. Furthermore everyone is also allowed to change and adjust it to ones needs.\newline \newline
MaryTTS is build in a "modular architecture" which means that it consists of different components, each dealing with a specific task. We will take a closer look at these in the following subchapter.
The second part of the name states the purpose Mary was developed for.
It was developed for research having a modular structure and also being a open source project makes it really easy to use Mary for any research projects on speech synthesis. \newline \newline
Mary is a multilingual TSS, featuring the languages English (British as well as American), German, Telugu, Turkish, Russian, Swedish and Italian. 
Further languages might be included in the future, but can also be added by any user. \newline \newline

\subsection{Modules}

Mary is subdivided into different modules. 
This allows the user to take a look at the output of each of those components. 
One could also interchange small details on one component and observe the output on the complete system as well as completely interchanging a component.
In most cases these components were not specifically designed for Mary. 
It would go beyond the scope of this paper to mention all of them. 
Detailed information can be found on the website \url{http://mary.dfki.de/ }.

\subsubsection*{Input}

Mary accepts different kinds of inputs, the simplest one being plain text.
But also Mark-up Languages such as SABLE or SSML can be used. 
These are based on the Mark-up Language XML and are developed especially for the use in TTS.

In case of the elevator, we only pass the output of the OpenDial dialogue manager to Mary, which is plain text.
Any input given to Mary will be transformed into a "MaryXML Mark-up Skeleton". 
A skeleton which already has different slots which will be filled by the system in the other modules.
If the input was already in a form of Mark-up language the keys assigned in this Mark-up Language will be kept and added to the skeleton. Some of the values however might be dropped.

\subsubsection*{Tokenizer}

The tokenizer identifies the tokens of a text. The output of this module will be sentences and words which will be marked as such.
Any token will be enclosed by "\textless t\textgreater ...\textless  t\textgreater". 
These marks are however just placeholder. 
Later on, further information will be saved in these slots.
Analogous a sentence is enclosed by "\textless s\textgreater ... \textless s\textgreater". 
Marking a dot as a sentence might seem obvious at first, but can prove quite difficult if there are other tokens in the text which also come with a dot, but do not mark the end of a sentence, such as the dot placed after titles.

\subsubsection*{Preprocessing}

This module, also called "Text-normalisation" does only work on very specific tokens.
It is only needed for acronyms or numbers.
For acronyms, the system will have to decide whether to pronounce them as one word or spell each letter out separately.
Numbers are divided into ordinal- and cardinal numbers.

\subsubsection*{Tagger and Chunker}

This module has two different tasks to solve. 
The Chunker labels the constituents in each sentence.
These are distinguished between adjective, nominal and prepositional phrase.  
A statistical POS-tagger working with trigram probabilities \footnote {To assign the correct word class the tagger does not only consider the current word but also the two words which were used last, making it a total of three words.} is used in order to assign every word its proper word class.

The findings of the two components are saved within the placeholders that were created by the tokenizer.
Unfortunately this also means that some of the information given by the chunker will be lost. This would be the case for local ambiguities.


\subsubsection*{Phonemisation}

This module also describes the work of several smaller steps.
The output of this system will be the transcription of every word into phonetic transcription.
To able to do this however there still needs to be some information extracted.
One of the steps needed is called "inflection endings". 
In this process, the tokens marked in the step "Preprocessing" are reviewed.
With the information given by the Chunker in the earlier steps as well as under the use of an morphologic analyser these tokens such as ordinal number can now be expanded.\newline 
In this step Mary will also check a dictionary specifically designed for Mary by the DFKI.
It contains information about the stem of a word as well grammatical information, which reduces the amount of data as regular words can be conjugated and inflected according to those rules. \newline
In case there is no information found about a word in the dictionary the "letter to sound conversion" will be used.
The output of this module only contains the SAMPA transcription and a note in case a word was processed by the "Letter to sound conversion". 
Information on intonation will not be seen in the output.

\subsubsection*{Prosody rules}

After the last module has assessed the intonation of a word, this module will analyse the prosody of the entire text.
The end of a sentence is a classic example for the intonation of a sentence. One of the most important aspect of prosody of a text are the pauses which have to be added to it. 
These will not only be added at the end of sentences but also at the end of paragraphs or chunks within a sentence.
The work of the chunker and tagger is therefore of great importance for this module.
\newline
Though not all of the information will be seen in the output of this module, every word will contain a new field called "accent" in its placeholder, where the information about the intonation of the word will be stored.

\subsubsection*{Postlexical and Phonological process}

In this step the data will be revised and everything will be put into context.
Smaller details such as glottal stops or elisions at the end of a word are being added. 
After this step the skeleton which has been created at the beginning of the process is filled with the maximum of information being available to Mary.

\subsubsection*{Calculation of acoustic parameters} 

This module will abandon the structure of the MaryXML structure.
Instead, the output will be a list of segments, their duration and the corresponding F0 value \footnote {The F0 value is a number describing the format 0, or the fundamental frequency of a voice and indicates the pitch.}

\subsubsection*{Synthesis}
In this final step, the values acquired in earlier steps will at last be converted into sound.
The modular architecture of Mary makes it easy to interchange the synthesis module as long as it supports the format of the output given by the previous module.
Currently MBROLA (Dutroit et al., 1996) is used in by Mary, which was selected as it creates a low distortion in the signal.

\newpage
\section{Raspberry Pi}
To implement the project, a Raspberry Pi is used instead of a conventional computer. The Raspberry Pi is a single-board computers 
developed in the United Kingdom by the Raspberry Pi Foundation. We used the \textit{RPi 2 model B}. 
Figure ~\ref{fig:Pi2} shows the RPi 2. 
\begin{figure}[ht]
\centering
\includegraphics[width=10.0cm]{Pi2.jpg}
\caption{\textit{An image showing Raspberry Pi 2 Model B used for the implementation of our project.}}
\label{fig:Pi2}
\end{figure}
This version of the Pi has the following specifications.
\begin{itemize}
  \item A 900MHz quad-core ARM Cortex-A7 CPU
  \item 1GB RAM
  \item 4 USB ports
  \item 40 GPIO pins
  \item Full HDMI port
  \item Ethernet port
  \item Combined 3.5mm audio jack and composite video
  \item Camera interface (CSI)
  \item Display interface (DSI)
  \item Micro SD card slot
  \item VideoCore IV 3D graphics core
\end{itemize}
\subsection{Raspberry Pi virtualization}
By setting up a virtual Raspberry Pi, the software can be further tested without physically accessing the Raspberry Pi and burning disk images all the time, even after all automatic tests have passed.
Raspberry Pi uses an ARM processor, so the only tool available to emulate it on PCs is \texttt{qemu}.

Basically follow the instructions from \url{http://www.linux-mitterteich.de/fileadmin/datafile/papers/2013/qemu_raspiemu_lug_18_sep_2013.pdf} but use the latest Raspbian image instead, which can be downloaded from \url{https://www.raspberrypi.org/downloads/}.
And download \texttt{kernel-qemu} from \url{http://web.archive.org/web/20150214035104/http://www.xecdesign.com/downloads/linux-qemu/kernel-qemu} because the original page is unavailable.

Currently this solution has several drawbacks, mostly notable ones are low speed and lack of serial port support.

\vspace{\baselineskip}

Note:

\begin{itemize}
\item The second line of \texttt{/etc/udev/rules.d/90-qemu.rules} is indeed \texttt{KERNEL=="sda?", SYMLINK+="mmcblk0p\%n",}, there is a typo in the picture.
\item Although the new RPi 2 model has an ARMv7 processor, Raspbian is still compiled into ARMv6 (for compatibility with old models), so emulating with \texttt{-cpu arm1176} is reasonable.
\end{itemize}

\section{Serial control of the elevator}
The Elevator is controlled by sending a hex encoded signal via serial cable.
In order to move the elevator to floor three, we are sending a signal which is interpreted as a move command to floor three.
We can also send a special "heartbeat" signal that will make the elevator respond with the information about the current floor it is on.
So, when the user enters the elevator cabin and asks to take him to certain floor, the user utterance is recognized and the corresponding floor signal is sent.
After the elevator receives the signal it should close the doors and start moving if everything went well.

% we need a few pics here when we finish the setup
%\begin{figure}
%\center{\includegraphics[scale=0.10, angle=-90]{setup_reverberation_rec.jpg}}
%\caption{Recordings setup.}
%\label{fig:recordingsetup}
%\end{figure}

\begin{comment}
\begin{figure}%
\parbox{1.2in}{%
\includegraphics[height=4.0cm, angle=-90]{inside_ellevator1.jpg}
\caption{First.}%
\label{fig:2figsA}}%
\qquad 
\hspace{10pt}
\begin{minipage}{1.2in}%
\includegraphics[height=4.0cm, angle=-90]{inside_ellevator3.jpg}
\caption{Second.}%
\label{fig:2figsB}%
\end{minipage}%
\end{figure}%

\begin{figure*}%[t!]
\centering
\begin{subfigure}[b]{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth, angle=-90]{inside_ellevator1.jpg}
\caption{Lorem ipsum}
\end{subfigure}
~ 
\begin{subfigure}[b]{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth, angle=-90]{inside_ellevator3.jpg}
\caption{Lorem ipsum, lorem ipsum,Lorem ipsum, lorem ipsum,Lorem ipsum}
\end{subfigure}
\caption{Caption place holder}
\end{figure*}
\end{comment}

\todo{TODO: a few images of the setup.}
Before working on serial implementation we reviewed several java libraries for serial communication: pi4j, javax.comm, RXTX.
It was decided to stay with RXTX, since pi4j was not stable enough and javax.comm didn't have freely available sources.
Serial implementation follows mainly RXTX examples provided \href{http://rxtx.qbang.org/wiki/index.php/Two_way_communcation_with_the_serial_port}{online} as well as source files of the original project.
For example, floor hex codes and "heartbeat" signal were taken from the original source code.
We also reused several methods from old source files, like the ones that deal with "heartbeat" processing logic.

\subsection{Serial cable connection setup}
Raspberri Pi 2 should be connected to the elevator via DE-9 type serial port.
After numerous of unsuccessful attempts to connect Raspi to the serial port via its GPIO pins, we decided to buy a USB to serial converter. 
The new converter worked fine out-of-the-box.
We successfully sent test signals via serial port from Raspi to desktop PC and vice verse.
After running several tests between Raspi and desktop PC we decided to test serial connection to the elevator.

We managed to send a signal to the elevator via serial port however the elevator was able to move only to one specific floor (0.5 which corresponds to 1 floor).
The signal was interpreted by the elevator as a command to move to floor 1.
We suspect that this is due to insufficient power (5V) of Raspi USB to serial connector.
The signal gets garbled upon transmission.
We believe that if we manage to boost the serial port power up to 12V the connection should stabilize and work correctly.

\subsection{Serial connection impleentation}
Within EllaVator project you can find the following classes that handle serial connection and data transmission between Raspberry Pi and the elevator: \texttt{ElevatorController.java, SerialControllerInterface.java, SerialPortController.java}.\\
\texttt{ElevatorController.java} is basically a wrapper class that instantiates \texttt{SerialPortController} with predefined constant values for serial connection.
These values are important and should not be changed unless you decide to replace Raspberri Pi.\\
Serial port name = \textbf{\texttt{/dev/ttyAMA0}} default serial port on Raspberry Pi.\\
\href{https://en.wikipedia.org/wiki/Serial\_port\#Speed}{Baudrate} = \textbf{38400} proved to work well with our implementation.\\
\href{https://en.wikipedia.org/wiki/Serial\_port\#Data_bits}{Bits} = use \textbf{8} bits.\\
\href{https://en.wikipedia.org/wiki/Serial\_port\#Stop_bits}{Stopbits} = use \textbf{1}.\\
\href{https://en.wikipedia.org/wiki/Serial\_port\#Parity}{Data correction or Parity} = set to \textbf{NO\_PARITY}.\\

All the constants are are kept in \texttt{SerialControllerInterface.java} class.
After we instantiate \texttt{SerialPortController} the following public methods become available through \texttt{ElevatorController} class: \texttt{getCurrentFloor()} and \texttt{pushButton()}. 
Method \texttt{pushButton()} is invoked whenever we want elevator to move to specific floor.
It accepts integers from 0 to 5 where each integer corresponds to specific floor (see \texttt{SerialControllerInterface.java}). 
Method \texttt{getCurrentFloor()} should return the integer which represents current floor the elevator is on.
This method triggers \texttt{getReceivedBytes}, \texttt{findValidSubstrings} (taken from original code) and \texttt{checkMessage} (taken from original code).
Method \texttt{findValidSubstrings} triggers \texttt{addByteStringToArrayList}.
Each method is accompanied by extensive information, please read it first.

\subsection{Serail port testing}
It is technically impossible to test Raspi and elevator connection without opening the elevator control panel.
Therefore, for testing we used an old laptop with serial port and a desktop PC.
For testing purposes we wrote several python scripts, pi4j java testing tool and a small RXTX java implementation.
We also used such shell utilities as minicom.
\\

There were 3 testing phases.

1 testing:

We used python pyserial implementation to test serial connection between Raspi, laptop and PC.
After several unsuccessful attempts to use pi4j and GPIO pins connection, we switched to USB serial converter.
Raspi successfully sent the signals via serial port using USB converter.
We also managed to test the serial connection between a laptop and a desktop PC.
Everything worked as expected.
\\

2 testing:

We connected the laptop to the elevator via laptop serial port with a simple serial cord.
Small RXTX serial port utility was used to send the signals from the laptop.
Everything worked correctly and we managed to control the elevator from the laptop by sending the appropriate signals.
\\

3 testing:

We connected Raspi to the elevator using USB serial converter.
The same RXTX serial port utility was used to send the signals to the elevator.
However whatever floor we tried to send the elevator to it moved to floor 1. 
We suspect that Raspi simply does not have enough power to support serial connection to the elevator.


\newpage
\section{Development tools}
\subsection{Gradle}
Gradle\footnote{http://gradle.org/} is a build automation tool and simplifies the build process drastically.
Instead of having a readme file that explains how to build the project, building the project is as easy as executing \texttt{./gradlew build} on the command line.

Gradle is configured in the file \texttt{build.gradle}.
Although it looks like a configuration file, it is actually fully functional code written in Groovy\footnote{http://www.groovy-lang.org/}, a scripting language based on Java.

We included a Gradle wrapper in the repository.
This file called \texttt{gradlew} (and \texttt{gradle.bat} for windows) download and run the right Gradle version when executed.
It is recommended to use this file instead of any other locally installed Gradle, because it might be another version of Gradle.

The file \texttt{settings.gradle} specifies the gradle project structure by declaring several subprojects.
There is the opendial subproject, which is a copy of the dialog manager OpenDial\footnote{http://www.opendial-toolkit.net/}.
Next we have the sphinx4 subproject, consisting of sphinx4-core and sphinx4-data, which is a copy of the speech recognition software Sphinx4~\cite{Walker:2004:SFO:1698193}.
These subprojects are all forked on github from the originals, eventually adapted for our purposes and then included as a git submodule.
Finally we have a subproject called prompts, that is our own making.
It contains example audio prompts for our application, that are used for the acoustic model training and for testing.

\subsection{GitHub}
GitHub is a web platform for collaboration using git, which is a version control system. 
We used this system to allow several people work on the same project at the same time without overwriting each others changes.

For some of our dependencies (Sphinx, OpenDial) we used git submodules to include them into our project. 
This gave us the possibility to adapt the original code for our purposes if necessary.
In order to download these dependencies when you clone the repository, you must therefore execute
\begin{lstlisting}[language=bash]
git clone https://github.com/EllaVator/EllaVator.git --recursive
\end{lstlisting}
or init and update all of the subprojects individually.
\subsection{Continuous integration with Travis CI}

Continuous integration is based on the idea that all developers send their changes to a common code repository as soon as possible, as we did on GitHub. 
Additionally these changes should be tested before they are merged into the main code repository. 
In order to do this automatically, it is necessary to automate the build process completely, as we did using Gradle.
If the build succeeds, all unit and integration tests can be executed, which prevents non-working code to enter the main repository if the error is covered by a test case.
This procedure is much better than having each developer running the tests before committing the changes to the main repository for a couple of reasons.
Firstly it does not rely on the developer to run all the tests which can be easily forgotten or skipped because the developer does not see the whole effect his modification has.
Secondly the project is built in a “neutral” environment that should be a close copy of the production environment if possible. 
This way for example code that runs fine on the developer's Windows machine but not on Linux can be detected.

As it integrates seamlessly into GitHub, we decided to use Travis CI as our continuous integration service. Each pull request gets marked by colors: yellow means the build is still running, red means failed build or failed test, green says everything is OK. In each pull request there is a link to Travis that gives details about the build process, too.

Travis is configured by a file named \texttt{.travis.yml} in the root directory of the project. 
It is not even necessary to specify that the build process is handled by Gradle.
Travis can detect this automatically, given that we specify groovy as our project language (\texttt{language: groovy}).
We needed to use Java 8 because this is a minimum requirement for Opendial.
Also, Travis must install the \texttt{sox} command, which is used to extract single sentences from our test audio files in the prompts subproject.
Finally, we cache the gradle files to speed up the build process (otherwise the Gradle wrapper will download Gradle every time).

Each developer can add Travis to his own repositories, too.
That way failures can be detected even before sending a pull request.
Travis can be activated by signing up on \url{https://travis-ci.org/} using your GitHub account.
Then go to your profile page on Travis and activate the GitHub repositories you want to be watched (which must contain a \texttt{.travis.yml} file).
From then on every push to these repositories will trigger Travis to build and run tests and send an email about the result.

\newpage
\section{Testing}
Software testing is an essential process in modern software development, in particular in the process of quality assurance (QA) of software.
Generally there at least two types of tests: unit and integration tests.
While unit tests are thought to test small components (e.g. classes) in isolation, integration tests verify how these components work as a whole.

Seeing time constraints and also our lack of experience, we decided not to be as strict with testing, and we put a focus on integration testing.
All tests are placed in the \texttt{test} source set, i.e. under \texttt{src/test}.
There they can be found automatically by gradle when executing \texttt{./gradlew test} in the commandline.
We used TestNG\footnote{http://testng.org/doc/} as our testing framework.

As an example let us have a look at the test called \texttt{SpeechIOTest} in \texttt{src/test/groovy}.

The package relevant for testing is imported like this:
\begin{lstlisting}[language=java]
import org.testng.annotations.*
\end{lstlisting}
Annotations are markers for methods (and other language elements), that start with an @ sign.
For instance, you can see the \texttt{@Test} annotation in line 31.
This marks the following method as a test method which means that this method is called when the tests are run by TestNG or indirectly by gradle.
Usually these test methods also contain one or more \texttt{assert} statements like in our example
\begin{lstlisting}[language=java]
assert actual == expected
\end{lstlisting}
Asserting something means that the test will only succeed if the condition evaluates to true.
Here we assert that the string that was recognized by the ASR from an audio file is identical with the words that actually were spoken in that file.

In this case we have the additional parameter \texttt{dataProvider = 'expandedGrammar'} in the \texttt{@Test} annotation.
This indicates that the test has to be executed multiple times with different test data, which is a very useful feature of TestNG.
The string \texttt{expandedGrammar} refers to the method of this name, that will return the test data as an array.
Each item in this array is an array of parameters that will be passed to the test function.

\newpage
\section{Installation instructions}
These installation instructions have been tested on Ubuntu 14.04.

If you have not installed git on your system, you should do this first:
\begin{lstlisting}[language=bash]
sudo apt-get install git
\end{lstlisting}

This program is the version control system we used and helps to keep changes by different developers in sync.
For details see the section on Github.

Now you can download the project files using the \texttt{git clone} command.
Navigate to the folder where you want to place the project files and execute:
\begin{lstlisting}[language=bash]
git clone https://github.com/EllaVator/EllaVator.git --recursive
\end{lstlisting}

This will create a folder named \texttt{EllaVator} inside the current directory and put the downloaded files inside.
Please note that you should use the \texttt{--recursive} flag here in order to download all the nessecary files, including the subprojects.

Now enter the created folder:
\begin{lstlisting}[language=bash]
cd EllaVator
\end{lstlisting}

Our project requires Java 8. If you haven't installed it already, you can do so using the following commands:
\begin{lstlisting}[language=bash]
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
\end{lstlisting}

Now you can build or run the project using one of the following commands:
\begin{lstlisting}[language=bash]
./gradlew build
./gradlew run
\end{lstlisting}

\cleardoublepage
\bibliography{final_doc}
\bibliographystyle{unsrt}
\end{document}
